#!/usr/bin/env python3
"""
OpenSearch Anomaly Detection í”ŒëŸ¬ê·¸ì¸ í…ŒìŠ¤íŠ¸
==========================================

Anomaly Detection í”ŒëŸ¬ê·¸ì¸ì˜ ì£¼ìš” ê¸°ëŠ¥ë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:
1. ì‹œê³„ì—´ ì´ìƒ íƒì§€ ì„¤ì •
2. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
3. ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ
4. ê¸ˆìœµ ì—…ë¬´ ì‹œë‚˜ë¦¬ì˜¤ (ê±°ë˜ íŒ¨í„´, ì‹œìŠ¤í…œ ì„±ëŠ¥)

ì‘ì„±ì: KCB IT AI ì¶”ì§„ë‹¨
ë‚ ì§œ: 2025-08-12
"""

import json
import time
import datetime
import numpy as np
from opensearchpy import OpenSearch
from opensearchpy.exceptions import RequestError, NotFoundError
import warnings
warnings.filterwarnings('ignore')

class AnomalyDetectionTester:
    def __init__(self):
        """OpenSearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.client = OpenSearch(
            hosts=[{'host': 'localhost', 'port': 9200}],
            http_auth=('admin', 'Kcb2025opSLabAi9'),
            use_ssl=True,
            verify_certs=False,
            ssl_show_warn=False
        )
        print("ğŸ” Anomaly Detection í”ŒëŸ¬ê·¸ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 50)
    
    def check_anomaly_plugin_status(self):
        """Anomaly Detection í”ŒëŸ¬ê·¸ì¸ ìƒíƒœ í™•ì¸"""
        print("\nğŸ“‹ 1. Anomaly Detection í”ŒëŸ¬ê·¸ì¸ ìƒíƒœ í™•ì¸")
        print("-" * 35)
        
        try:
            # í”ŒëŸ¬ê·¸ì¸ ëª©ë¡ í™•ì¸
            response = self.client.cat.plugins(format='json', v=True)
            anomaly_plugins = [p for p in response if 'anomaly' in p['component'].lower()]
            
            if anomaly_plugins:
                print("âœ… Anomaly Detection í”ŒëŸ¬ê·¸ì¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤:")
                for plugin in anomaly_plugins:
                    print(f"   - {plugin['component']} v{plugin['version']}")
            else:
                print("âŒ Anomaly Detection í”ŒëŸ¬ê·¸ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ OpenSearch 2.13.0ì—ëŠ” ê¸°ë³¸ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
            
            # Anomaly Detection API ìƒíƒœ í™•ì¸
            try:
                detectors_response = self.client.transport.perform_request(
                    'GET',
                    '/_plugins/_anomaly_detection/detectors/_search',
                    body={"size": 0}
                )
                print(f"âœ… Anomaly Detection API ì •ìƒ ë™ì‘")
                print(f"ğŸ“Š ë“±ë¡ëœ íƒì§€ê¸° ìˆ˜: {detectors_response.get('hits', {}).get('total', {}).get('value', 0)}")
                
            except Exception as e:
                print(f"âš ï¸ Anomaly Detection API í™•ì¸ ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ ê³„ì† ì§„í–‰í•˜ì—¬ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸í•´ë³´ê² ìŠµë‹ˆë‹¤.")
            
            return True
            
        except Exception as e:
            print(f"âŒ í”ŒëŸ¬ê·¸ì¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def setup_financial_data(self):
        """ê¸ˆìœµ ì—…ë¬´ìš© ì‹œê³„ì—´ ë°ì´í„° ìƒì„±"""
        print("\nğŸ“Š 2. ê¸ˆìœµ ì‹œê³„ì—´ ë°ì´í„° ìƒì„±")
        print("-" * 30)
        
        try:
            # 1. ì¼ì¼ ê±°ë˜ëŸ‰ ë°ì´í„° ì¸ë±ìŠ¤
            transaction_volume_mapping = {
                "mappings": {
                    "properties": {
                        "@timestamp": {"type": "date"},
                        "date": {"type": "date"},
                        "daily_transaction_count": {"type": "long"},
                        "daily_transaction_amount": {"type": "double"},
                        "avg_transaction_size": {"type": "double"},
                        "peak_hour_transactions": {"type": "long"},
                        "system_load": {"type": "double"},
                        "response_time_ms": {"type": "double"},
                        "error_rate": {"type": "double"},
                        "region": {"type": "keyword"}
                    }
                }
            }
            
            # 2. ì‹¤ì‹œê°„ ê±°ë˜ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì¸ë±ìŠ¤
            realtime_transactions_mapping = {
                "mappings": {
                    "properties": {
                        "@timestamp": {"type": "date"},
                        "transaction_id": {"type": "keyword"},
                        "customer_id": {"type": "keyword"},
                        "amount": {"type": "double"},
                        "merchant_type": {"type": "keyword"},
                        "location": {"type": "keyword"},
                        "is_weekend": {"type": "boolean"},
                        "hour_of_day": {"type": "integer"},
                        "anomaly_score": {"type": "double"}
                    }
                }
            }
            
            # ì¸ë±ìŠ¤ ìƒì„±
            indices = [
                ("financial-daily-metrics", transaction_volume_mapping),
                ("financial-realtime-transactions", realtime_transactions_mapping)
            ]
            
            for index_name, mapping in indices:
                if self.client.indices.exists(index=index_name):
                    self.client.indices.delete(index=index_name)
                
                self.client.indices.create(index=index_name, body=mapping)
                print(f"âœ… ì¸ë±ìŠ¤ '{index_name}' ìƒì„± ì™„ë£Œ")
            
            # 30ì¼ê°„ ì¼ì¼ ë©”íŠ¸ë¦­ ë°ì´í„° ìƒì„±
            daily_data = []
            np.random.seed(42)
            base_date = datetime.datetime(2025, 7, 13)  # 30ì¼ ì „ë¶€í„°
            
            for day in range(30):
                current_date = base_date + datetime.timedelta(days=day)
                is_weekend = current_date.weekday() >= 5
                
                # ì£¼ë§/ì£¼ì¤‘ íŒ¨í„´ ë°˜ì˜
                if is_weekend:
                    base_transactions = 8000 + np.random.normal(0, 500)
                    base_amount = 12000000 + np.random.normal(0, 1000000)
                else:
                    base_transactions = 15000 + np.random.normal(0, 1000)
                    base_amount = 25000000 + np.random.normal(0, 2000000)
                
                # íŠ¹ì • ë‚ ì§œì— ì´ìƒ íŒ¨í„´ ì£¼ì… (ì‹œìŠ¤í…œ ì¥ì• , í”„ë¡œëª¨ì…˜ ë“±)
                if day in [5, 12, 20]:  # ì´ìƒ ë‚ ì§œ
                    if day == 5:  # ì‹œìŠ¤í…œ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
                        base_transactions *= 0.3
                        base_amount *= 0.3
                        system_load = 95 + np.random.normal(0, 3)
                        response_time = 2000 + np.random.normal(0, 500)
                        error_rate = 15 + np.random.normal(0, 3)
                    elif day == 12:  # í”„ë¡œëª¨ì…˜ ì´ë²¤íŠ¸
                        base_transactions *= 2.5
                        base_amount *= 3.0
                        system_load = 85 + np.random.normal(0, 5)
                        response_time = 800 + np.random.normal(0, 200)
                        error_rate = 2 + np.random.normal(0, 1)
                    else:  # ì™¸ë¶€ ê³µê²© ì‹œë®¬ë ˆì´ì…˜
                        base_transactions *= 1.8
                        base_amount *= 0.7
                        system_load = 90 + np.random.normal(0, 5)
                        response_time = 1500 + np.random.normal(0, 300)
                        error_rate = 8 + np.random.normal(0, 2)
                else:  # ì •ìƒ ë²”ìœ„
                    system_load = 45 + np.random.normal(0, 10)
                    response_time = 200 + np.random.normal(0, 50)
                    error_rate = 0.5 + np.random.normal(0, 0.3)
                
                daily_record = {
                    "@timestamp": current_date.isoformat(),
                    "date": current_date.strftime("%Y-%m-%d"),
                    "daily_transaction_count": max(1000, int(base_transactions)),
                    "daily_transaction_amount": max(1000000, base_amount),
                    "avg_transaction_size": base_amount / base_transactions if base_transactions > 0 else 0,
                    "peak_hour_transactions": max(500, int(base_transactions * 0.15)),
                    "system_load": max(0, min(100, system_load)),
                    "response_time_ms": max(50, response_time),
                    "error_rate": max(0, min(100, error_rate)),
                    "region": np.random.choice(["Seoul", "Busan", "Incheon", "Daegu", "Gwangju"])
                }
                daily_data.append(daily_record)
            
            # ì‹¤ì‹œê°„ ê±°ë˜ ë°ì´í„° ìƒì„± (ìµœê·¼ 3ì¼)
            realtime_data = []
            for day_offset in range(3):
                current_date = datetime.datetime.now() - datetime.timedelta(days=day_offset)
                
                # í•˜ë£¨ 24ì‹œê°„ ë™ì•ˆ ì‹œê°„ë‹¹ ê±°ë˜ ìƒì„±
                for hour in range(24):
                    transaction_time = current_date.replace(hour=hour, minute=0, second=0)
                    is_weekend = transaction_time.weekday() >= 5
                    
                    # ì‹œê°„ëŒ€ë³„ ê±°ë˜ íŒ¨í„´
                    if 6 <= hour <= 9:  # ì¶œê·¼ ì‹œê°„
                        transaction_count = 200 + np.random.poisson(50)
                    elif 12 <= hour <= 14:  # ì ì‹¬ ì‹œê°„
                        transaction_count = 300 + np.random.poisson(80)
                    elif 18 <= hour <= 22:  # í‡´ê·¼/ì €ë… ì‹œê°„
                        transaction_count = 250 + np.random.poisson(70)
                    else:  # ê¸°íƒ€ ì‹œê°„
                        transaction_count = 100 + np.random.poisson(30)
                    
                    # ì£¼ë§ íŒ¨í„´ ì¡°ì •
                    if is_weekend:
                        transaction_count = int(transaction_count * 0.7)
                    
                    # ê±°ë˜ ìƒì„±
                    for _ in range(transaction_count):
                        # ì •ìƒ ê±°ë˜ íŒ¨í„´
                        if np.random.random() > 0.02:  # 98% ì •ìƒ
                            if 9 <= hour <= 17:  # ì—…ë¬´ì‹œê°„
                                amount = np.random.lognormal(np.log(100), 0.8)
                            else:  # ë¹„ì—…ë¬´ì‹œê°„
                                amount = np.random.lognormal(np.log(50), 0.6)
                            anomaly_score = np.random.uniform(0, 0.3)
                        else:  # 2% ì´ìƒ ê±°ë˜
                            amount = np.random.lognormal(np.log(1000), 1.2)  # í° ê¸ˆì•¡
                            anomaly_score = np.random.uniform(0.7, 1.0)
                        
                        transaction = {
                            "@timestamp": (transaction_time + datetime.timedelta(
                                minutes=np.random.randint(0, 60),
                                seconds=np.random.randint(0, 60)
                            )).isoformat(),
                            "transaction_id": f"TXN_{day_offset:02d}{hour:02d}{np.random.randint(1000, 9999)}",
                            "customer_id": f"CUST_{np.random.randint(1, 1001):04d}",
                            "amount": amount,
                            "merchant_type": np.random.choice([
                                "restaurant", "grocery", "gas_station", "retail", 
                                "online", "pharmacy", "entertainment"
                            ]),
                            "location": np.random.choice([
                                "Seoul_Gangnam", "Seoul_Jongno", "Busan_Haeundae", 
                                "Incheon_Airport", "Daegu_Central"
                            ]),
                            "is_weekend": is_weekend,
                            "hour_of_day": hour,
                            "anomaly_score": anomaly_score
                        }
                        realtime_data.append(transaction)
            
            # ë²Œí¬ ì¸ë±ì‹±
            print(f"ğŸ“Š ì¼ì¼ ë©”íŠ¸ë¦­ ë°ì´í„° ì¸ë±ì‹± ì¤‘... ({len(daily_data)}ê±´)")
            bulk_body = []
            for record in daily_data:
                bulk_body.extend([
                    {"index": {"_index": "financial-daily-metrics"}},
                    record
                ])
            
            self.client.bulk(body=bulk_body, refresh=True)
            
            print(f"ğŸ“Š ì‹¤ì‹œê°„ ê±°ë˜ ë°ì´í„° ì¸ë±ì‹± ì¤‘... ({len(realtime_data)}ê±´)")
            bulk_body = []
            for record in realtime_data:
                bulk_body.extend([
                    {"index": {"_index": "financial-realtime-transactions"}},
                    record
                ])
            
            self.client.bulk(body=bulk_body, refresh=True)
            
            print(f"âœ… ì´ {len(daily_data)}ê°œ ì¼ì¼ ë©”íŠ¸ë¦­ + {len(realtime_data)}ê°œ ì‹¤ì‹œê°„ ê±°ë˜ ë°ì´í„° ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ê¸ˆìœµ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def create_anomaly_detectors(self):
        """ì´ìƒ íƒì§€ê¸° ìƒì„± ë° ì„¤ì •"""
        print("\nğŸ¯ 3. ì´ìƒ íƒì§€ê¸° ìƒì„± ë° ì„¤ì •")
        print("-" * 30)
        
        try:
            # 1. ì¼ì¼ ê±°ë˜ëŸ‰ ì´ìƒ íƒì§€ê¸°
            daily_detector_config = {
                "name": "daily_transaction_anomaly_detector",
                "description": "ì¼ì¼ ê±°ë˜ëŸ‰ ë° ì‹œìŠ¤í…œ ì„±ëŠ¥ ì´ìƒ íƒì§€",
                "time_field": "@timestamp",
                "indices": ["financial-daily-metrics"],
                "detection_interval": {
                    "period": {
                        "interval": 1,
                        "unit": "Minutes"
                    }
                },
                "window_delay": {
                    "period": {
                        "interval": 1,
                        "unit": "Minutes"
                    }
                },
                "shingle_size": 8,
                "schema_version": 0,
                "feature_attributes": [
                    {
                        "feature_id": "daily_transactions",
                        "feature_name": "daily_transaction_count",
                        "feature_enabled": True,
                        "aggregation_query": {
                            "daily_transactions": {
                                "sum": {
                                    "field": "daily_transaction_count"
                                }
                            }
                        }
                    },
                    {
                        "feature_id": "system_performance",
                        "feature_name": "system_load_avg",
                        "feature_enabled": True,
                        "aggregation_query": {
                            "system_performance": {
                                "avg": {
                                    "field": "system_load"
                                }
                            }
                        }
                    },
                    {
                        "feature_id": "response_time",
                        "feature_name": "avg_response_time",
                        "feature_enabled": True,
                        "aggregation_query": {
                            "response_time": {
                                "avg": {
                                    "field": "response_time_ms"
                                }
                            }
                        }
                    }
                ]
            }
            
            # 2. ì‹¤ì‹œê°„ ê±°ë˜ ì´ìƒ íƒì§€ê¸°
            realtime_detector_config = {
                "name": "realtime_transaction_anomaly_detector",
                "description": "ì‹¤ì‹œê°„ ê±°ë˜ íŒ¨í„´ ì´ìƒ íƒì§€",
                "time_field": "@timestamp",
                "indices": ["financial-realtime-transactions"],
                "detection_interval": {
                    "period": {
                        "interval": 1,
                        "unit": "Minutes"
                    }
                },
                "window_delay": {
                    "period": {
                        "interval": 1,
                        "unit": "Minutes"
                    }
                },
                "shingle_size": 4,
                "schema_version": 0,
                "feature_attributes": [
                    {
                        "feature_id": "transaction_volume",
                        "feature_name": "hourly_transaction_count",
                        "feature_enabled": True,
                        "aggregation_query": {
                            "transaction_volume": {
                                "value_count": {
                                    "field": "transaction_id"
                                }
                            }
                        }
                    },
                    {
                        "feature_id": "avg_amount",
                        "feature_name": "average_transaction_amount",
                        "feature_enabled": True,
                        "aggregation_query": {
                            "avg_amount": {
                                "avg": {
                                    "field": "amount"
                                }
                            }
                        }
                    },
                    {
                        "feature_id": "high_value_transactions",
                        "feature_name": "large_transaction_count",
                        "feature_enabled": True,
                        "aggregation_query": {
                            "high_value_transactions": {
                                "value_count": {
                                    "field": "amount",
                                    "script": {
                                        "source": "doc['amount'].value > 500"
                                    }
                                }
                            }
                        }
                    }
                ]
            }
            
            # íƒì§€ê¸° ìƒì„±
            detectors = [
                ("daily", daily_detector_config),
                ("realtime", realtime_detector_config)
            ]
            
            created_detectors = {}
            
            for detector_type, config in detectors:
                try:
                    response = self.client.transport.perform_request(
                        'POST',
                        '/_plugins/_anomaly_detection/detectors',
                        body=config
                    )
                    
                    detector_id = response.get('_id')
                    created_detectors[detector_type] = detector_id
                    print(f"âœ… {detector_type} íƒì§€ê¸° ìƒì„± ì„±ê³µ (ID: {detector_id})")
                    
                    # íƒì§€ê¸° ì‹œì‘
                    start_response = self.client.transport.perform_request(
                        'POST',
                        f'/_plugins/_anomaly_detection/detectors/{detector_id}/_start'
                    )
                    print(f"âœ… {detector_type} íƒì§€ê¸° ì‹œì‘ë¨")
                    
                except Exception as e:
                    print(f"âš ï¸ {detector_type} íƒì§€ê¸° ìƒì„±/ì‹œì‘ ì‹¤íŒ¨: {e}")
                    # ê¸°ì¡´ íƒì§€ê¸°ê°€ ìˆëŠ”ì§€ í™•ì¸
                    try:
                        search_response = self.client.transport.perform_request(
                            'GET',
                            '/_plugins/_anomaly_detection/detectors/_search',
                            body={
                                "query": {
                                    "match": {
                                        "name": config["name"]
                                    }
                                }
                            }
                        )
                        
                        if search_response.get('hits', {}).get('hits'):
                            existing_detector = search_response['hits']['hits'][0]
                            detector_id = existing_detector['_id']
                            created_detectors[detector_type] = detector_id
                            print(f"ğŸ“‹ ê¸°ì¡´ {detector_type} íƒì§€ê¸° ë°œê²¬ (ID: {detector_id})")
                        
                    except Exception as search_error:
                        print(f"âŒ {detector_type} íƒì§€ê¸° ê²€ìƒ‰ ì‹¤íŒ¨: {search_error}")
            
            if created_detectors:
                print(f"\nğŸ“Š ìƒì„±ëœ íƒì§€ê¸° ìˆ˜: {len(created_detectors)}")
                self.detector_ids = created_detectors
                return True
            else:
                print("âŒ ìƒì„±ëœ íƒì§€ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
        except Exception as e:
            print(f"âŒ ì´ìƒ íƒì§€ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def analyze_anomaly_results(self):
        """ì´ìƒ íƒì§€ ê²°ê³¼ ë¶„ì„"""
        print("\nğŸ“ˆ 4. ì´ìƒ íƒì§€ ê²°ê³¼ ë¶„ì„")
        print("-" * 25)
        
        try:
            # ê¸°ë³¸ í†µê³„ ê¸°ë°˜ ì´ìƒ íƒì§€ ë¶„ì„
            print("ğŸ“Š ê¸°ë³¸ í†µê³„ ê¸°ë°˜ ì´ìƒ íŒ¨í„´ ë¶„ì„:")
            
            # 1. ì¼ì¼ ë©”íŠ¸ë¦­ ì´ìƒ íŒ¨í„´ ë¶„ì„
            daily_analysis_query = {
                "size": 0,
                "aggs": {
                    "daily_stats": {
                        "date_histogram": {
                            "field": "@timestamp",
                            "calendar_interval": "1d"
                        },
                        "aggs": {
                            "total_transactions": {"sum": {"field": "daily_transaction_count"}},
                            "avg_system_load": {"avg": {"field": "system_load"}},
                            "avg_response_time": {"avg": {"field": "response_time_ms"}},
                            "avg_error_rate": {"avg": {"field": "error_rate"}},
                            "anomaly_score": {
                                "bucket_script": {
                                    "buckets_path": {
                                        "load": "avg_system_load",
                                        "response": "avg_response_time",
                                        "errors": "avg_error_rate"
                                    },
                                    "script": """
                                        double load_score = params.load > 80 ? (params.load - 80) / 20.0 : 0;
                                        double response_score = params.response > 1000 ? (params.response - 1000) / 1000.0 : 0;
                                        double error_score = params.errors > 5 ? (params.errors - 5) / 10.0 : 0;
                                        return Math.min(1.0, (load_score + response_score + error_score) / 3.0);
                                    """
                                }
                            }
                        }
                    },
                    "overall_stats": {
                        "stats": {"field": "daily_transaction_count"}
                    }
                }
            }
            
            daily_result = self.client.search(
                index="financial-daily-metrics", 
                body=daily_analysis_query
            )
            
            print("\n   ğŸ“… ì¼ì¼ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„:")
            anomalous_days = []
            for bucket in daily_result['aggregations']['daily_stats']['buckets']:
                date = bucket['key_as_string'][:10]
                transactions = bucket['total_transactions']['value']
                system_load = bucket['avg_system_load']['value']
                response_time = bucket['avg_response_time']['value']
                error_rate = bucket['avg_error_rate']['value']
                anomaly_score = bucket['anomaly_score']['value']
                
                status = "ğŸš¨ ì´ìƒ" if anomaly_score > 0.3 else "âœ… ì •ìƒ"
                print(f"     {date}: {status} (ì ìˆ˜: {anomaly_score:.3f})")
                print(f"       - ê±°ë˜ëŸ‰: {transactions:,.0f}, ì‹œìŠ¤í…œ ë¶€í•˜: {system_load:.1f}%")
                print(f"       - ì‘ë‹µì‹œê°„: {response_time:.0f}ms, ì˜¤ë¥˜ìœ¨: {error_rate:.2f}%")
                
                if anomaly_score > 0.3:
                    anomalous_days.append({
                        "date": date,
                        "score": anomaly_score,
                        "transactions": transactions,
                        "system_load": system_load,
                        "response_time": response_time,
                        "error_rate": error_rate
                    })
            
            # 2. ì‹¤ì‹œê°„ ê±°ë˜ íŒ¨í„´ ë¶„ì„
            realtime_analysis_query = {
                "size": 0,
                "aggs": {
                    "hourly_patterns": {
                        "date_histogram": {
                            "field": "@timestamp",
                            "calendar_interval": "1h"
                        },
                        "aggs": {
                            "transaction_count": {"value_count": {"field": "transaction_id"}},
                            "avg_amount": {"avg": {"field": "amount"}},
                            "high_value_count": {
                                "filter": {
                                    "range": {"amount": {"gte": 500}}
                                }
                            },
                            "avg_anomaly_score": {"avg": {"field": "anomaly_score"}},
                            "suspicious_transactions": {
                                "filter": {
                                    "range": {"anomaly_score": {"gte": 0.7}}
                                }
                            }
                        }
                    },
                    "merchant_anomalies": {
                        "terms": {
                            "field": "merchant_type",
                            "size": 10
                        },
                        "aggs": {
                            "avg_anomaly_score": {"avg": {"field": "anomaly_score"}},
                            "high_risk_transactions": {
                                "filter": {
                                    "range": {"anomaly_score": {"gte": 0.5}}
                                }
                            }
                        }
                    }
                }
            }
            
            realtime_result = self.client.search(
                index="financial-realtime-transactions",
                body=realtime_analysis_query
            )
            
            print(f"\n   â° ì‹œê°„ëŒ€ë³„ ê±°ë˜ íŒ¨í„´ ë¶„ì„:")
            suspicious_hours = []
            for bucket in realtime_result['aggregations']['hourly_patterns']['buckets']:
                timestamp = bucket['key_as_string']
                count = bucket['transaction_count']['value']
                avg_amount = bucket['avg_amount']['value']
                high_value_count = bucket['high_value_count']['doc_count']
                avg_anomaly = bucket['avg_anomaly_score']['value']
                suspicious_count = bucket['suspicious_transactions']['doc_count']
                
                if suspicious_count > 0 or avg_anomaly > 0.3:
                    suspicious_hours.append({
                        "time": timestamp,
                        "suspicious_count": suspicious_count,
                        "avg_anomaly": avg_anomaly
                    })
                    status = "ğŸš¨"
                else:
                    status = "âœ…"
                
                print(f"     {timestamp}: {status} {count}ê±´ ê±°ë˜ (í‰ê· : ${avg_amount:.0f})")
                if suspicious_count > 0:
                    print(f"       - ì˜ì‹¬ ê±°ë˜: {suspicious_count}ê±´, í‰ê·  ì´ìƒì ìˆ˜: {avg_anomaly:.3f}")
            
            print(f"\n   ğŸª ì—…ì¢…ë³„ ì´ìƒ íŒ¨í„´ ë¶„ì„:")
            for bucket in realtime_result['aggregations']['merchant_anomalies']['buckets']:
                merchant = bucket['key']
                avg_anomaly = bucket['avg_anomaly_score']['value']
                high_risk_count = bucket['high_risk_transactions']['doc_count']
                
                risk_level = "ğŸš¨ ë†’ìŒ" if avg_anomaly > 0.4 else "âš ï¸ ì¤‘ê°„" if avg_anomaly > 0.2 else "âœ… ë‚®ìŒ"
                print(f"     {merchant}: {risk_level} (í‰ê·  ì´ìƒì ìˆ˜: {avg_anomaly:.3f}, ê³ ìœ„í—˜: {high_risk_count}ê±´)")
            
            # 3. ìš”ì•½ ë³´ê³ ì„œ
            print(f"\nğŸ“‹ ì´ìƒ íƒì§€ ìš”ì•½ ë³´ê³ ì„œ:")
            print(f"   ğŸ—“ï¸ ì´ìƒ ê°ì§€ëœ ë‚ ì§œ: {len(anomalous_days)}ì¼")
            print(f"   â° ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì‹œê°„ëŒ€: {len(suspicious_hours)}íšŒ")
            
            if anomalous_days:
                worst_day = max(anomalous_days, key=lambda x: x['score'])
                print(f"   ğŸ“‰ ìµœê³  ìœ„í—˜ ë‚ ì§œ: {worst_day['date']} (ì ìˆ˜: {worst_day['score']:.3f})")
                print(f"     - ì£¼ìš” ì´ìŠˆ: ", end="")
                issues = []
                if worst_day['system_load'] > 80:
                    issues.append(f"ë†’ì€ ì‹œìŠ¤í…œ ë¶€í•˜({worst_day['system_load']:.1f}%)")
                if worst_day['response_time'] > 1000:
                    issues.append(f"ëŠë¦° ì‘ë‹µì‹œê°„({worst_day['response_time']:.0f}ms)")
                if worst_day['error_rate'] > 5:
                    issues.append(f"ë†’ì€ ì˜¤ë¥˜ìœ¨({worst_day['error_rate']:.2f}%)")
                print(", ".join(issues) if issues else "ë³µí•©ì  ì´ìƒ íŒ¨í„´")
            
            return True
            
        except Exception as e:
            print(f"âŒ ì´ìƒ íƒì§€ ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return False
    
    def test_real_time_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("\nâ±ï¸ 5. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("-" * 30)
        
        try:
            # ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œìš© ì¿¼ë¦¬ë“¤
            monitoring_queries = {
                "current_system_status": {
                    "size": 1,
                    "sort": [{"@timestamp": {"order": "desc"}}],
                    "query": {"match_all": {}},
                    "_source": ["@timestamp", "system_load", "response_time_ms", "error_rate"]
                },
                "last_hour_transactions": {
                    "size": 0,
                    "query": {
                        "range": {
                            "@timestamp": {
                                "gte": "now-1h"
                            }
                        }
                    },
                    "aggs": {
                        "total_transactions": {"value_count": {"field": "transaction_id"}},
                        "total_amount": {"sum": {"field": "amount"}},
                        "avg_amount": {"avg": {"field": "amount"}},
                        "suspicious_transactions": {
                            "filter": {
                                "range": {"anomaly_score": {"gte": 0.5}}
                            }
                        },
                        "by_merchant": {
                            "terms": {"field": "merchant_type", "size": 5},
                            "aggs": {
                                "transaction_count": {"value_count": {"field": "transaction_id"}},
                                "avg_anomaly": {"avg": {"field": "anomaly_score"}}
                            }
                        }
                    }
                },
                "performance_trends": {
                    "size": 0,
                    "query": {
                        "range": {
                            "@timestamp": {
                                "gte": "now-24h"
                            }
                        }
                    },
                    "aggs": {
                        "hourly_performance": {
                            "date_histogram": {
                                "field": "@timestamp",
                                "calendar_interval": "1h"
                            },
                            "aggs": {
                                "avg_system_load": {"avg": {"field": "system_load"}},
                                "avg_response_time": {"avg": {"field": "response_time_ms"}},
                                "avg_error_rate": {"avg": {"field": "error_rate"}}
                            }
                        }
                    }
                }
            }
            
            print("ğŸ“Š ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ìƒíƒœ:")
            
            # 1. í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ
            try:
                current_status = self.client.search(
                    index="financial-daily-metrics",
                    body=monitoring_queries["current_system_status"]
                )
                
                if current_status['hits']['hits']:
                    latest = current_status['hits']['hits'][0]['_source']
                    system_load = latest['system_load']
                    response_time = latest['response_time_ms']
                    error_rate = latest['error_rate']
                    
                    # ìƒíƒœ í‰ê°€
                    load_status = "ğŸš¨ ìœ„í—˜" if system_load > 80 else "âš ï¸ ì£¼ì˜" if system_load > 60 else "âœ… ì •ìƒ"
                    response_status = "ğŸš¨ ìœ„í—˜" if response_time > 1000 else "âš ï¸ ì£¼ì˜" if response_time > 500 else "âœ… ì •ìƒ"
                    error_status = "ğŸš¨ ìœ„í—˜" if error_rate > 5 else "âš ï¸ ì£¼ì˜" if error_rate > 2 else "âœ… ì •ìƒ"
                    
                    print(f"   ì‹œìŠ¤í…œ ë¶€í•˜: {system_load:.1f}% {load_status}")
                    print(f"   ì‘ë‹µ ì‹œê°„: {response_time:.0f}ms {response_status}")
                    print(f"   ì˜¤ë¥˜ìœ¨: {error_rate:.2f}% {error_status}")
                else:
                    print("   âŒ í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ë°ì´í„° ì—†ìŒ")
                    
            except Exception as e:
                print(f"   âŒ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # 2. ìµœê·¼ 1ì‹œê°„ ê±°ë˜ í˜„í™©
            try:
                recent_transactions = self.client.search(
                    index="financial-realtime-transactions",
                    body=monitoring_queries["last_hour_transactions"]
                )
                
                aggs = recent_transactions['aggregations']
                total_count = aggs['total_transactions']['value']
                total_amount = aggs['total_amount']['value']
                avg_amount = aggs['avg_amount']['value']
                suspicious_count = aggs['suspicious_transactions']['doc_count']
                
                print(f"\nğŸ“ˆ ìµœê·¼ 1ì‹œê°„ ê±°ë˜ í˜„í™©:")
                print(f"   ì´ ê±°ë˜ ê±´ìˆ˜: {total_count:,.0f}ê±´")
                print(f"   ì´ ê±°ë˜ ê¸ˆì•¡: ${total_amount:,.0f}")
                print(f"   í‰ê·  ê±°ë˜ ê¸ˆì•¡: ${avg_amount:.0f}")
                print(f"   ì˜ì‹¬ ê±°ë˜: {suspicious_count}ê±´ ({suspicious_count/total_count*100:.1f}%)")
                
                print(f"\n   ì—…ì¢…ë³„ ê±°ë˜ í˜„í™©:")
                for bucket in aggs['by_merchant']['buckets']:
                    merchant = bucket['key']
                    count = bucket['transaction_count']['value']
                    avg_anomaly = bucket['avg_anomaly']['value']
                    risk_indicator = "ğŸš¨" if avg_anomaly > 0.3 else "âš ï¸" if avg_anomaly > 0.15 else "âœ…"
                    print(f"     {merchant}: {count:.0f}ê±´ {risk_indicator} (ì´ìƒì ìˆ˜: {avg_anomaly:.3f})")
                    
            except Exception as e:
                print(f"   âŒ ê±°ë˜ í˜„í™© ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # 3. 24ì‹œê°„ ì„±ëŠ¥ íŠ¸ë Œë“œ
            try:
                performance_trends = self.client.search(
                    index="financial-daily-metrics",
                    body=monitoring_queries["performance_trends"]
                )
                
                print(f"\nğŸ“Š 24ì‹œê°„ ì„±ëŠ¥ íŠ¸ë Œë“œ:")
                trend_data = []
                for bucket in performance_trends['aggregations']['hourly_performance']['buckets']:
                    if bucket['doc_count'] > 0:
                        hour_data = {
                            'time': bucket['key_as_string'],
                            'load': bucket['avg_system_load']['value'],
                            'response': bucket['avg_response_time']['value'],
                            'errors': bucket['avg_error_rate']['value']
                        }
                        trend_data.append(hour_data)
                
                if trend_data:
                    # ìµœê·¼ ëª‡ ì‹œê°„ ë°ì´í„° í‘œì‹œ
                    recent_trends = trend_data[-6:]  # ìµœê·¼ 6ì‹œê°„
                    for data in recent_trends:
                        time_str = data['time'][11:16]  # HH:MM í˜•ì‹
                        load_trend = "ğŸ“ˆ" if data['load'] > 70 else "ğŸ“Š" if data['load'] > 40 else "ğŸ“‰"
                        print(f"     {time_str}: {load_trend} ë¶€í•˜ {data['load']:.1f}%, ì‘ë‹µ {data['response']:.0f}ms")
                    
                    # íŠ¸ë Œë“œ ë¶„ì„
                    avg_load = sum(d['load'] for d in recent_trends) / len(recent_trends)
                    avg_response = sum(d['response'] for d in recent_trends) / len(recent_trends)
                    
                    print(f"\n   ğŸ“Š ìµœê·¼ 6ì‹œê°„ í‰ê· :")
                    print(f"     ì‹œìŠ¤í…œ ë¶€í•˜: {avg_load:.1f}%")
                    print(f"     ì‘ë‹µ ì‹œê°„: {avg_response:.0f}ms")
                    
                    # ì„±ëŠ¥ ì˜ˆì¸¡
                    if len(recent_trends) >= 3:
                        load_trend = recent_trends[-1]['load'] - recent_trends[-3]['load']
                        response_trend = recent_trends[-1]['response'] - recent_trends[-3]['response']
                        
                        print(f"\n   ğŸ”® ì„±ëŠ¥ ì˜ˆì¸¡:")
                        if load_trend > 10:
                            print("     âš ï¸ ì‹œìŠ¤í…œ ë¶€í•˜ ì¦ê°€ ì¶”ì„¸ - ìš©ëŸ‰ í™•ì¥ ê²€í†  í•„ìš”")
                        elif load_trend < -10:
                            print("     âœ… ì‹œìŠ¤í…œ ë¶€í•˜ ê°ì†Œ ì¶”ì„¸ - ì•ˆì •ì ")
                        else:
                            print("     ğŸ“Š ì‹œìŠ¤í…œ ë¶€í•˜ ì•ˆì •ì ")
                            
                        if response_trend > 200:
                            print("     âš ï¸ ì‘ë‹µ ì‹œê°„ ì¦ê°€ ì¶”ì„¸ - ì„±ëŠ¥ ìµœì í™” í•„ìš”")
                        elif response_trend < -200:
                            print("     âœ… ì‘ë‹µ ì‹œê°„ ê°œì„  ì¶”ì„¸")
                        else:
                            print("     ğŸ“Š ì‘ë‹µ ì‹œê°„ ì•ˆì •ì ")
                else:
                    print("   âŒ íŠ¸ë Œë“œ ë°ì´í„° ì—†ìŒ")
                    
            except Exception as e:
                print(f"   âŒ ì„±ëŠ¥ íŠ¸ë Œë“œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # 4. ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œë®¬ë ˆì´ì…˜
            print(f"\nğŸš¨ ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œë®¬ë ˆì´ì…˜:")
            
            # ì„ê³„ê°’ ì²´í¬
            alerts = []
            try:
                # ìµœê·¼ ë°ì´í„°ë¡œ ì•Œë¦¼ ì¡°ê±´ ì²´í¬
                alert_query = {
                    "size": 10,
                    "sort": [{"@timestamp": {"order": "desc"}}],
                    "query": {
                        "bool": {
                            "should": [
                                {"range": {"system_load": {"gte": 80}}},
                                {"range": {"response_time_ms": {"gte": 1000}}},
                                {"range": {"error_rate": {"gte": 5}}}
                            ]
                        }
                    }
                }
                
                alert_results = self.client.search(
                    index="financial-daily-metrics",
                    body=alert_query
                )
                
                for hit in alert_results['hits']['hits']:
                    source = hit['_source']
                    timestamp = source['@timestamp']
                    
                    if source['system_load'] >= 80:
                        alerts.append(f"ğŸš¨ HIGH_LOAD: ì‹œìŠ¤í…œ ë¶€í•˜ {source['system_load']:.1f}% ({timestamp})")
                    if source['response_time_ms'] >= 1000:
                        alerts.append(f"ğŸš¨ SLOW_RESPONSE: ì‘ë‹µì‹œê°„ {source['response_time_ms']:.0f}ms ({timestamp})")
                    if source['error_rate'] >= 5:
                        alerts.append(f"ğŸš¨ HIGH_ERROR: ì˜¤ë¥˜ìœ¨ {source['error_rate']:.2f}% ({timestamp})")
                
                # ì˜ì‹¬ ê±°ë˜ ì•Œë¦¼
                suspicious_query = {
                    "size": 5,
                    "sort": [{"@timestamp": {"order": "desc"}}],
                    "query": {
                        "range": {"anomaly_score": {"gte": 0.8}}
                    }
                }
                
                suspicious_results = self.client.search(
                    index="financial-realtime-transactions",
                    body=suspicious_query
                )
                
                for hit in suspicious_results['hits']['hits']:
                    source = hit['_source']
                    alerts.append(f"ğŸš¨ FRAUD_ALERT: ì˜ì‹¬ê±°ë˜ ${source['amount']:.0f} "
                                f"(ê³ ê°: {source['customer_id']}, ì ìˆ˜: {source['anomaly_score']:.3f})")
                
            except Exception as e:
                alerts.append(f"âŒ ì•Œë¦¼ ì²´í¬ ì‹¤íŒ¨: {e}")
            
            if alerts:
                print("   ë°œìƒí•œ ì•Œë¦¼:")
                for alert in alerts[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    print(f"     {alert}")
                if len(alerts) > 5:
                    print(f"     ... ì™¸ {len(alerts)-5}ê°œ ì•Œë¦¼")
            else:
                print("   âœ… í˜„ì¬ ë°œìƒí•œ ì•Œë¦¼ ì—†ìŒ")
            
            return True
            
        except Exception as e:
            print(f"âŒ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    def generate_business_recommendations(self):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ì¶”ì²œ ì‚¬í•­ ìƒì„±"""
        print("\nğŸ’¡ 6. ë¹„ì¦ˆë‹ˆìŠ¤ ì¶”ì²œ ì‚¬í•­ ìƒì„±")
        print("-" * 30)
        
        try:
            # ì¢…í•© ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
            analysis_queries = {
                "system_reliability": {
                    "size": 0,
                    "aggs": {
                        "avg_uptime": {
                            "avg": {
                                "script": {
                                    "source": "100 - doc['error_rate'].value"
                                }
                            }
                        },
                        "performance_distribution": {
                            "range": {
                                "field": "response_time_ms",
                                "ranges": [
                                    {"key": "excellent", "to": 200},
                                    {"key": "good", "from": 200, "to": 500},
                                    {"key": "poor", "from": 500, "to": 1000},
                                    {"key": "critical", "from": 1000}
                                ]
                            }
                        },
                        "load_distribution": {
                            "histogram": {
                                "field": "system_load",
                                "interval": 20
                            }
                        }
                    }
                },
                "transaction_insights": {
                    "size": 0,
                    "aggs": {
                        "fraud_rate": {
                            "avg": {"field": "anomaly_score"}
                        },
                        "high_risk_merchants": {
                            "terms": {
                                "field": "merchant_type",
                                "order": {"avg_anomaly": "desc"}
                            },
                            "aggs": {
                                "avg_anomaly": {"avg": {"field": "anomaly_score"}},
                                "transaction_count": {"value_count": {"field": "transaction_id"}}
                            }
                        },
                        "peak_hours": {
                            "terms": {
                                "field": "hour_of_day",
                                "size": 24,
                                "order": {"_count": "desc"}
                            }
                        },
                        "weekend_vs_weekday": {
                            "terms": {"field": "is_weekend"},
                            "aggs": {
                                "avg_amount": {"avg": {"field": "amount"}},
                                "avg_anomaly": {"avg": {"field": "anomaly_score"}}
                            }
                        }
                    }
                }
            }
            
            # ì‹œìŠ¤í…œ ì‹ ë¢°ì„± ë¶„ì„
            system_result = self.client.search(
                index="financial-daily-metrics",
                body=analysis_queries["system_reliability"]
            )
            
            # ê±°ë˜ ì¸ì‚¬ì´íŠ¸ ë¶„ì„
            transaction_result = self.client.search(
                index="financial-realtime-transactions",
                body=analysis_queries["transaction_insights"]
            )
            
            print("ğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼:")
            
            # ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„
            avg_uptime = system_result['aggregations']['avg_uptime']['value']
            perf_dist = system_result['aggregations']['performance_distribution']['buckets']
            
            print(f"\n   ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€:")
            print(f"     í‰ê·  ê°€ìš©ì„±: {avg_uptime:.2f}%")
            
            total_samples = sum(bucket['doc_count'] for bucket in perf_dist)
            for bucket in perf_dist:
                percentage = (bucket['doc_count'] / total_samples * 100) if total_samples > 0 else 0
                print(f"     {bucket['key']} ì„±ëŠ¥: {percentage:.1f}%")
            
            # ê±°ë˜ íŒ¨í„´ ë¶„ì„
            fraud_rate = transaction_result['aggregations']['fraud_rate']['value']
            high_risk_merchants = transaction_result['aggregations']['high_risk_merchants']['buckets']
            peak_hours = transaction_result['aggregations']['peak_hours']['buckets'][:3]
            weekend_analysis = transaction_result['aggregations']['weekend_vs_weekday']['buckets']
            
            print(f"\n   ğŸ’³ ê±°ë˜ íŒ¨í„´ ë¶„ì„:")
            print(f"     ì „ì²´ ì´ìƒ ì ìˆ˜: {fraud_rate:.3f}")
            
            print(f"     ê³ ìœ„í—˜ ì—…ì¢… TOP 3:")
            for i, bucket in enumerate(high_risk_merchants[:3], 1):
                merchant = bucket['key']
                avg_anomaly = bucket['avg_anomaly']['value']
                count = bucket['transaction_count']['value']
                print(f"       {i}. {merchant}: {avg_anomaly:.3f} ({count:.0f}ê±´)")
            
            print(f"     ê±°ë˜ ì§‘ì¤‘ ì‹œê°„ëŒ€:")
            for bucket in peak_hours:
                hour = bucket['key']
                count = bucket['doc_count']
                print(f"       {hour:02d}:00 - {count}ê±´")
            
            # ë¹„ì¦ˆë‹ˆìŠ¤ ì¶”ì²œì‚¬í•­ ìƒì„±
            print(f"\nğŸ¯ ë¹„ì¦ˆë‹ˆìŠ¤ ì¶”ì²œì‚¬í•­:")
            
            recommendations = []
            
            # ì‹œìŠ¤í…œ ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œ
            if avg_uptime < 95:
                recommendations.append("ğŸ”§ ì‹œìŠ¤í…œ ì•ˆì •ì„± ê°œì„  í•„ìš” - SLA ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ì¸í”„ë¼ ê°•í™”")
            
            excellent_perf = next((b['doc_count'] for b in perf_dist if b['key'] == 'excellent'), 0)
            total_perf = sum(b['doc_count'] for b in perf_dist)
            excellent_ratio = (excellent_perf / total_perf * 100) if total_perf > 0 else 0
            
            if excellent_ratio < 70:
                recommendations.append("âš¡ ì‘ë‹µì†ë„ ìµœì í™” - ìš°ìˆ˜ ì„±ëŠ¥ ë¹„ìœ¨ì„ 70% ì´ìƒìœ¼ë¡œ ê°œì„ ")
            
            # ë³´ì•ˆ ë° ì‚¬ê¸° ë°©ì§€ ì¶”ì²œ
            if fraud_rate > 0.3:
                recommendations.append("ğŸ›¡ï¸ ì‚¬ê¸° íƒì§€ ì‹œìŠ¤í…œ ê°•í™” - ì´ìƒ ì ìˆ˜ê°€ ë†’ì€ ê±°ë˜ íŒ¨í„´ ë¶„ì„ ë° ëŒ€ì‘")
            
            # ê³ ìœ„í—˜ ì—…ì¢…ë³„ ë§ì¶¤ ëŒ€ì‘
            if high_risk_merchants and high_risk_merchants[0]['avg_anomaly']['value'] > 0.4:
                top_risk_merchant = high_risk_merchants[0]['key']
                recommendations.append(f"ğŸ¯ {top_risk_merchant} ì—…ì¢… íŠ¹ë³„ ëª¨ë‹ˆí„°ë§ - ì¶”ê°€ ë³´ì•ˆ ì¡°ì¹˜ ì ìš©")
            
            # ìš´ì˜ íš¨ìœ¨ì„± ê°œì„ 
            weekend_data = {bucket['key']: bucket for bucket in weekend_analysis}
            if weekend_data.get(True, {}).get('avg_anomaly', {}).get('value', 0) > weekend_data.get(False, {}).get('avg_anomaly', {}).get('value', 0):
                recommendations.append("ğŸ“… ì£¼ë§ ë³´ì•ˆ ê°•í™” - ì£¼ë§ ì´ìƒ ê±°ë˜ ë¹„ìœ¨ì´ í‰ì¼ë³´ë‹¤ ë†’ìŒ")
            
            # ìš©ëŸ‰ ê³„íš
            peak_hour_traffic = max(bucket['doc_count'] for bucket in peak_hours) if peak_hours else 0
            avg_hour_traffic = sum(bucket['doc_count'] for bucket in peak_hours) / len(peak_hours) if peak_hours else 0
            
            if peak_hour_traffic > avg_hour_traffic * 2:
                recommendations.append("ğŸ“ˆ íŠ¸ë˜í”½ ë¶„ì‚° ìµœì í™” - í”¼í¬ ì‹œê°„ëŒ€ ë¶€í•˜ ë¶„ì‚° ë°©ì•ˆ ê²€í† ")
            
            # KCB íŠ¹í™” ì¶”ì²œì‚¬í•­
            kcb_recommendations = [
                "ğŸ’¼ ì‹ ìš©í‰ê°€ ì •í™•ë„ í–¥ìƒ - ì´ìƒ ê±°ë˜ íŒ¨í„´ì„ ì‹ ìš©í‰ê°€ ëª¨ë¸ì— ë°˜ì˜",
                "ğŸ”— íƒ€ ì‹œìŠ¤í…œ ì—°ë™ ê°•í™” - ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ ê²°ê³¼ë¥¼ CRM/ERP ì‹œìŠ¤í…œê³¼ ì—°ê³„",
                "ğŸ“Š ì˜ˆì¸¡ ë¶„ì„ ë„ì… - ê³¼ê±° íŒ¨í„´ ê¸°ë°˜ ë¯¸ë˜ ì´ìƒ ìƒí™© ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•",
                "ğŸ“ ì§ì› êµìœ¡ í”„ë¡œê·¸ë¨ - ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ í™œìš©ë²• ë° ëŒ€ì‘ ì ˆì°¨ êµìœ¡"
            ]
            
            recommendations.extend(kcb_recommendations[:2])  # ìƒìœ„ 2ê°œ ì¶”ê°€
            
            # ì¶”ì²œì‚¬í•­ ì¶œë ¥
            for i, rec in enumerate(recommendations, 1):
                print(f"     {i}. {rec}")
            
            # êµ¬í˜„ ìš°ì„ ìˆœìœ„
            print(f"\nğŸ“‹ êµ¬í˜„ ìš°ì„ ìˆœìœ„:")
            priority_items = [
                ("High", "ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•", "ì¦‰ì‹œ ëŒ€ì‘ ê°€ëŠ¥í•œ ëª¨ë‹ˆí„°ë§"),
                ("High", "ìë™ ì´ìƒ íƒì§€ ì •ì±… ìˆ˜ë¦½", "ì—…ë¬´ ê·œì¹™ ê¸°ë°˜ ì„ê³„ê°’ ì„¤ì •"),
                ("Medium", "ëŒ€ì‹œë³´ë“œ ê³ ë„í™”", "ê²½ì˜ì§„ ë¦¬í¬íŒ…ìš© ì‹œê°í™”"),
                ("Medium", "ì˜ˆì¸¡ ëª¨ë¸ ë„ì…", "proactive ì´ìƒ íƒì§€"),
                ("Low", "ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ë™", "í†µí•© ëª¨ë‹ˆí„°ë§ í™˜ê²½ êµ¬ì¶•")
            ]
            
            for priority, item, description in priority_items:
                priority_icon = "ğŸ”´" if priority == "High" else "ğŸŸ¡" if priority == "Medium" else "ğŸŸ¢"
                print(f"     {priority_icon} {priority}: {item}")
                print(f"       â†’ {description}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ë¹„ì¦ˆë‹ˆìŠ¤ ì¶”ì²œì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def run_all_tests(self):
        """ëª¨ë“  Anomaly Detection í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ OpenSearch Anomaly Detection í”ŒëŸ¬ê·¸ì¸ ì¢…í•© í…ŒìŠ¤íŠ¸")
        print("=" * 65)
        
        test_results = {}
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_methods = [
            ("í”ŒëŸ¬ê·¸ì¸ ìƒíƒœ í™•ì¸", self.check_anomaly_plugin_status),
            ("ê¸ˆìœµ ë°ì´í„° ìƒì„±", self.setup_financial_data),
            ("ì´ìƒ íƒì§€ê¸° ìƒì„±", self.create_anomaly_detectors),
            ("ì´ìƒ íƒì§€ ê²°ê³¼ ë¶„ì„", self.analyze_anomaly_results),
            ("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§", self.test_real_time_monitoring),
            ("ë¹„ì¦ˆë‹ˆìŠ¤ ì¶”ì²œì‚¬í•­", self.generate_business_recommendations)
        ]
        
        for test_name, test_method in test_methods:
            try:
                result = test_method()
                test_results[test_name] = result
                if result:
                    print(f"\nâœ… {test_name} í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                else:
                    print(f"\nâŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            except Exception as e:
                print(f"\nğŸ’¥ {test_name} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                test_results[test_name] = False
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print(f"\n" + "=" * 65)
        print("ğŸ“‹ Anomaly Detection í”ŒëŸ¬ê·¸ì¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 65)
        
        passed_tests = sum(1 for result in test_results.values() if result)
        total_tests = len(test_results)
        
        for test_name, result in test_results.items():
            status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
            print(f"   {test_name}: {status}")
        
        print(f"\nğŸ¯ ì„±ê³µë¥ : {passed_tests}/{total_tests} ({passed_tests/total_tests*100:.1f}%)")
        
        if passed_tests == total_tests:
            print("ğŸ‰ ëª¨ë“  Anomaly Detection í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # í™œìš© ê°€ì´ë“œ
        print(f"\n" + "=" * 65)
        print("ğŸ’¡ Anomaly Detection í™œìš© ê°€ì´ë“œ (KCB ë§ì¶¤)")
        print("=" * 65)
        print("ğŸ¯ ê¸ˆìœµì—… ì´ìƒ íƒì§€ í™œìš© ì‚¬ë¡€:")
        print("   1. ì‹¤ì‹œê°„ ê±°ë˜ ëª¨ë‹ˆí„°ë§")
        print("      - ì¹´ë“œ ê²°ì œ ì´ìƒ íŒ¨í„´ íƒì§€")
        print("      - ê³„ì¢Œ ì´ì²´ ì‚¬ê¸° ë°©ì§€")
        print("      - ATM ê±°ë˜ ëª¨ë‹ˆí„°ë§")
        print()
        print("   2. ì‹œìŠ¤í…œ ì„±ëŠ¥ ê´€ë¦¬")
        print("      - ì„œë²„ ì„±ëŠ¥ ì´ìƒ ê°ì§€")
        print("      - ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ëª¨ë‹ˆí„°ë§")
        print("      - ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ì¶”ì ")
        print()
        print("   3. ê³ ê° í–‰ë™ ë¶„ì„")
        print("      - ë¹„ì •ìƒ ë¡œê·¸ì¸ íŒ¨í„´")
        print("      - ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê³„ì¢Œ í™œë™")
        print("      - ì‹ ìš©ì¹´ë“œ ë„ìš© íƒì§€")
        print()
        print("   4. ê·œì œ ì¤€ìˆ˜ ë° ë¦¬ìŠ¤í¬ ê´€ë¦¬")
        print("      - AML(ìê¸ˆì„¸íƒë°©ì§€) ëª¨ë‹ˆí„°ë§")
        print("      - ë‚´ë¶€ í†µì œ ì‹œìŠ¤í…œ")
        print("      - ìš´ì˜ ë¦¬ìŠ¤í¬ ê´€ë¦¬")
        print()
        print("ğŸ”§ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:")
        print("   1. Alerting í”ŒëŸ¬ê·¸ì¸ìœ¼ë¡œ ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•")
        print("   2. ì‹¤ì œ ì—…ë¬´ ë°ì´í„° ì—°ë™ í…ŒìŠ¤íŠ¸")
        print("   3. ëŒ€ì‹œë³´ë“œ êµ¬ì¶• (Observability í”ŒëŸ¬ê·¸ì¸)")
        print("   4. ì„±ëŠ¥ ìµœì í™” (Performance Analyzer í”ŒëŸ¬ê·¸ì¸)")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = AnomalyDetectionTester()
    tester.run_all_tests()


if __name__ == "__main__":
    main()